# README - Dual-Contril-DRL-Meal-Delivery

This repository supports the research paper titled **Real-Time Integrated Dispatching and Idle Fleet Steering with Deep Reinforcement Learning for A Meal Delivery Platform (2025, under review)**. \
**Authors**: Jingyi Cheng* and Shadi Sharif Azadeh.\
**Institute**: Transport and Planning, Delft University of Technology.\
The manuscript is under review at _Data Science for Transportation_. 

## Abstract: 
To achieve high service quality and profitability, meal delivery platforms like Uber Eats and Grubhub must strategically operate their fleets to ensure timely deliveries for current orders while mitigating the consequential impacts of suboptimal decisions that leads to courier understaffing in the future.
This study set out to solve the real-time order dispatching and idle courier steering problems for a meal delivery platform by proposing a reinforcement learning (RL)-based strategic dual-control framework. 
To address the inherent sequential nature of these problems, we model both dispatching and steering as Markov Decision Processes. Trained via a deep reinforcement learning (DRL) framework, we obtain strategic policies by leveraging the explicitly predicted demands as part of the inputs. In our dual-control framework, the dispatching and steering policies are iteratively trained in an integrated manner. These forward-looking policies can be executed in real-time and provide decisions while jointly considering the impacts on local and network levels. To enhance dispatching fairness, we propose convolutional deep Q networks to construct fair courier embeddings. To simultaneously rebalance the supply and demand within the service network, we propose to utilize mean-field approximated supply-demand knowledge to reallocate idle couriers at the local level. 
Utilizing the policies generated by the RL-based strategic dual-control framework, we find the delivery efficiency and fairness of workload distribution among couriers have been improved, and under-supplied conditions have been alleviated within the service network.
Our study sheds light on designing an RL-based framework to enable forward-looking real-time operations for meal delivery platforms and other on-demand services

## Research Description

This research introduces an innovative **Reinforcement Learning (RL)-Based Strategic Dual-Control Framework**, which encompasses:

1. **Order Dispatching**: Real-time allocation of couriers to orders using convolutional deep Q-networks, ensuring enhanced fairness and operational efficiency.
2. **Idle Fleet Steering**: Adaptive rebalancing of idle couriers across the service network using a mean-field approximation approach to address disparities between supply and demand.

Both components are modeled as Markov Decision Processes and trained in an iterative and integrated manner. By incorporating predicted demand as part of the input, the framework facilitates real-time execution of forward-looking policies that significantly improve delivery efficiency and equitable workload distribution among couriers.

## Instructions

### Prerequisites

- Python (>=3.10) installed with all dependencies listed in `requirements.txt`.
- Access to a GPU for expedited training is recommended but optional.

### Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/rl-dual-control-meal-delivery.git
   cd rl-dual-control-meal-delivery
   ```
2. Install required dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Configure the settings as needed:
   - Adjust the configuration files in the `config/` directory (e.g., `settings.yaml`).

### Running the Code

1. Train the model:
   ```bash
   python src/train.py --config config/settings.yaml
   ```
2. Evaluate the model:
   ```bash
   python src/evaluate.py --model checkpoints/best_model.pth
   ```
3. Visualize the results:
   ```bash
   python scripts/visualize_results.py
   ```

## Repository Structure

```
project-root/
├── README.md               # Project description and setup instructions
├── src/                    # Source code for training and evaluation
│   ├── train.py
│   ├── evaluate.py
│   └── model/
├── tests/                  # Test scripts
│   └── test_main_code_file.py
├── config/                 # Configuration files
│   └── settings.yaml
├── docs/                   # Documentation
│   └── architecture.md
├── data/                   # Sample datasets
│   └── sample_data.csv
├── images/                 # Images for documentation
│   └── logo.png
├── scripts/                # Utility scripts
│   └── visualize_results.py
├── .gitignore              # Files to ignore
├── papers/                 # Relevant publications
├── presentations/          # Slides and other presentations
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.
